{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCRShV8glg4o"
      },
      "source": [
        "# **Prediction of Genetic Associations in ALS Through NLP and Complex Network Analysis**\n",
        "\n",
        "This research project aims to predict candidate genes in Amyotrophic Lateral Sclerosis (ALS) using Natural Language Processing (NLP) and complex network analysis techniques.\n",
        "\n",
        "- **Author:** João Pedro Viguini T. T. Correa  \n",
        "- **Supervisor:** Prof. Dr. Ricardo Cerri\n",
        "\n",
        "This research is supported by FAPESP (2025/06512-0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwXGWiJKX6RW"
      },
      "source": [
        "# Package Installation\n",
        "\n",
        "- **[IMPORTANT]** Please ensure that all necessary files are downloaded from the GitHub repository and uploaded to your Google Drive.\n",
        "\n",
        "- The steps below detail the installation process for the required packages and models used in this study.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8EXAVpDKura",
        "outputId": "c26e453d-ca92-4ede-c88e-046eadb80801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "EvEHVm6DX8n3",
        "outputId": "ae31ec0d-c7bd-4289-e9bc-442abc9324df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==3.7.5 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 2))\n",
            "  Downloading spacy-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 3)) (2.2.2)\n",
            "Collecting beautifulsoup4==4.13.4 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 4))\n",
            "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting biopython==1.85 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 5))\n",
            "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting fasttext==0.9.3 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 6))\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.12/dist-packages (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 7)) (6.0.2)\n",
            "Collecting scikit_learn==1.7.0 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 8))\n",
            "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting setuptools==80.6.0 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 9))\n",
            "  Downloading setuptools-80.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting mygene (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 10))\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (4.0.0)\n",
            "Requirement already satisfied: opencv-python>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 13)) (4.12.0.88)\n",
            "Requirement already satisfied: opencv-contrib-python>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 14)) (4.12.0.88)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 15)) (4.12.0.88)\n",
            "Collecting albumentations==1.3.1 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 17))\n",
            "  Downloading albumentations-1.3.1-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: requests==2.32.4 in /usr/local/lib/python3.12/dist-packages (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 19)) (2.32.4)\n",
            "Collecting rich==13.7.1 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 20))\n",
            "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting jedi>=0.16 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 21))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (3.0.10)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2))\n",
            "  Downloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (0.17.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->-r ./drive/MyDrive/IC_2025/requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->-r ./drive/MyDrive/IC_2025/requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->-r ./drive/MyDrive/IC_2025/requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4==4.13.4->-r ./drive/MyDrive/IC_2025/requirements.txt (line 4)) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4==4.13.4->-r ./drive/MyDrive/IC_2025/requirements.txt (line 4)) (4.15.0)\n",
            "Collecting pybind11>=2.2 (from fasttext==0.9.3->-r ./drive/MyDrive/IC_2025/requirements.txt (line 6))\n",
            "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn==1.7.0->-r ./drive/MyDrive/IC_2025/requirements.txt (line 8)) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn==1.7.0->-r ./drive/MyDrive/IC_2025/requirements.txt (line 8)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn==1.7.0->-r ./drive/MyDrive/IC_2025/requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.3.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 17)) (0.25.2)\n",
            "Collecting qudida>=0.0.4 (from albumentations==1.3.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 17))\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4->-r ./drive/MyDrive/IC_2025/requirements.txt (line 19)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4->-r ./drive/MyDrive/IC_2025/requirements.txt (line 19)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4->-r ./drive/MyDrive/IC_2025/requirements.txt (line 19)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4->-r ./drive/MyDrive/IC_2025/requirements.txt (line 19)) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich==13.7.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 20)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich==13.7.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 20)) (2.19.2)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->-r ./drive/MyDrive/IC_2025/requirements.txt (line 10))\n",
            "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (0.35.0)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python>=4.9.0.80 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 13))\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python>=4.9.0.80 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 14))\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python-headless>=4.9.0.80 (from -r ./drive/MyDrive/IC_2025/requirements.txt (line 15))\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->-r ./drive/MyDrive/IC_2025/requirements.txt (line 21)) (0.8.5)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from biothings-client>=0.2.6->mygene->-r ./drive/MyDrive/IC_2025/requirements.txt (line 10)) (0.28.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (1.1.10)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich==13.7.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 20)) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ./drive/MyDrive/IC_2025/requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 17)) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 17)) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 17)) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 17)) (2025.9.9)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->-r ./drive/MyDrive/IC_2025/requirements.txt (line 17)) (0.4)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2))\n",
            "  Downloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r ./drive/MyDrive/IC_2025/requirements.txt (line 11)) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->-r ./drive/MyDrive/IC_2025/requirements.txt (line 10)) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->-r ./drive/MyDrive/IC_2025/requirements.txt (line 10)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->-r ./drive/MyDrive/IC_2025/requirements.txt (line 10)) (0.16.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r ./drive/MyDrive/IC_2025/requirements.txt (line 2)) (1.17.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->-r ./drive/MyDrive/IC_2025/requirements.txt (line 10)) (1.3.1)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Downloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp312-cp312-linux_x86_64.whl size=4498202 sha256=69f901e7862ccedb0f973a821e719ef2fcceef4f0b630c04df5a19fc875945ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/27/95/a7baf1b435f1cbde017cabdf1e9688526d2b0e929255a359c6\n",
            "Successfully built fasttext\n",
            "Installing collected packages: setuptools, pybind11, numpy, jedi, beautifulsoup4, rich, opencv-python-headless, opencv-python, opencv-contrib-python, fasttext, blis, biopython, scikit_learn, biothings-client, thinc, qudida, mygene, spacy, albumentations\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.5\n",
            "    Uninstalling beautifulsoup4-4.13.5:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.5\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.12.0.88\n",
            "    Uninstalling opencv-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-4.12.0.88\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.7\n",
            "    Uninstalling spacy-3.8.7:\n",
            "      Successfully uninstalled spacy-3.8.7\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.8\n",
            "    Uninstalling albumentations-2.0.8:\n",
            "      Successfully uninstalled albumentations-2.0.8\n",
            "Successfully installed albumentations-1.3.1 beautifulsoup4-4.13.4 biopython-1.85 biothings-client-0.4.1 blis-0.7.11 fasttext-0.9.3 jedi-0.19.2 mygene-3.2.2 numpy-1.26.4 opencv-contrib-python-4.11.0.86 opencv-python-4.11.0.86 opencv-python-headless-4.11.0.86 pybind11-3.0.1 qudida-0.0.4 rich-13.7.1 scikit_learn-1.7.0 setuptools-80.6.0 spacy-3.7.5 thinc-8.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "numpy"
                ]
              },
              "id": "d16e7891ebc84d019d05ed18d7218fcc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r ./drive/MyDrive/IC_2025/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrJBjo6KQhtV"
      },
      "source": [
        "Install spaCy models for NER and tokenization --> It should be inside your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hjzEHLncdb4S",
        "outputId": "ad21020c-9c8b-4d19-fee7-2b148e0e3cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./drive/MyDrive/IC_2025/en_ner_bionlp13cg_md-0.5.4.tar.gz\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: en_ner_bionlp13cg_md\n",
            "  Building wheel for en_ner_bionlp13cg_md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en_ner_bionlp13cg_md: filename=en_ner_bionlp13cg_md-0.5.4-py3-none-any.whl size=119814705 sha256=0bc22cf48855631d47cdb126527dbdefc6e2f4b131bea7268a4041437a8e022d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/fb/8b/4bbf308c03bde2232c00f76c958d4e7bcf7b4f2874c9b2159c\n",
            "Successfully built en_ner_bionlp13cg_md\n",
            "Installing collected packages: en_ner_bionlp13cg_md\n",
            "Successfully installed en_ner_bionlp13cg_md-0.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade drive/MyDrive/IC_2025/en_ner_bionlp13cg_md-0.5.4.tar.gz --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFsjZ3Kw70JU",
        "outputId": "dcd851cf-a86c-4900-be6d-bdbfb3e07f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Y3KilBXn2Q"
      },
      "source": [
        "# Import\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLrio4Xmbx7V"
      },
      "source": [
        "**Restart the session before running this cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpQm1mMnMGAR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Bio import Entrez\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    import mygene\n",
        "    MYGENE_AVAILABLE = True\n",
        "except Exception:\n",
        "    print(\"mygene not available\")\n",
        "    MYGENE_AVAILABLE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZBf5PIkMIqm"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7F20QW8MKQs"
      },
      "outputs": [],
      "source": [
        "Entrez.email = \"jpvviguini@gmail.com\" # replace\n",
        "API_KEY = \"7edcb0657d8ffc045a7eec1068abad863b09\"   # replace\n",
        "YEAR_START = 2000\n",
        "YEAR_END = 2010\n",
        "MAX_ARTICLES = 100 # how many articles you want to retrieve\n",
        "SLEEP_TIME = 0.37\n",
        "MAX_RETRIES = 3\n",
        "CHUNK_SIZE = 200\n",
        "VALIDATE_WITH_MYGENE = True\n",
        "MYGENE_BATCH_SIZE = 1000\n",
        "\n",
        "\n",
        "BASE_QUERY = (\n",
        "    '(\"amyotrophic lateral sclerosis\"[tiab] OR \"motor neuron disease\"[tiab] OR MND[tiab] OR ALS[tiab]) AND '\n",
        "    '(\"gene\"[tiab] OR \"genes\"[tiab] OR genetic[tiab] OR mutation*[tiab] OR polymorphism*[tiab] OR \"Genome-Wide Association Study\"[Mesh] OR GWAS[tiab])'\n",
        "\n",
        ")\n",
        "# )\n",
        "# BASE_QUERY = (\n",
        "#   '( \"gene[tiab]\" OR \"genes[tiab]\" OR genetic[tiab] '\n",
        "#   'OR mutation*[tiab] OR polymorphism*[tiab] '\n",
        "#   'OR variant*[tiab] OR SNP[tiab] OR SNPs[tiab] '\n",
        "#   'OR loci[tiab] OR locus[tiab] '\n",
        "#   'OR GWAS[tiab] OR \"genome-wide association\"[tiab] '\n",
        "#   'OR expression[tiab] ) '\n",
        "#   'AND '\n",
        "#   '( association*[tiab] OR relationship*[tiab] '\n",
        "#   'OR correlation*[tiab] OR interaction*[tiab] '\n",
        "#   'OR linkage[tiab] OR \"risk factor*\"[tiab] '\n",
        "#   'OR susceptib*[tiab] OR regulat*[tiab] )'\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm52yncVMUX2"
      },
      "source": [
        "# Loading NLP models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Wb2Jh6MVzJ",
        "outputId": "162b4b63-9d39-4913-c3d9-34d74ba38893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading NLP models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
            "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading NLP models...\")\n",
        "nlp_ner = None\n",
        "\n",
        "# spaCy NER\n",
        "try:\n",
        "    nlp_ner = spacy.load(\"en_ner_bionlp13cg_md\", disable=[\"tagger\", \"parser\"])\n",
        "except Exception as e:\n",
        "    print(\"Warning: could not load en_ner_bionlp13cg_md (NER). Error:\", e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XACFzndMqmy"
      },
      "source": [
        "# Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsZpmVygMsf3"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s\\-]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "\n",
        "GENE_STOPWORDS = set([\n",
        "    \"THE\", \"AND\", \"WITH\", \"FOR\", \"WAS\", \"WERE\", \"ARE\", \"OUR\", \"FROM\",\n",
        "    \"THIS\", \"THAT\", \"THAN\", \"DISEASE\", \"PATIENT\", \"PATIENTS\", \"GENETIC\", \"RISK\",\n",
        "    \"STUDY\", \"GENE\", \"GENES\", \"ANALYSIS\", \"RESULT\", \"RESULTS\", \"DATA\", \"MODEL\",\n",
        "    \"MODELS\", \"TYPE\", \"CASE\", \"CASES\", \"ALS\", \"LATERAL\", \"SCLEROSIS\", \"MOTOR\",\n",
        "    \"NEURON\", \"DNA\", \"RNA\", \"PROTEIN\", \"CELL\", \"CELLS\", \"TISSUE\", \"BRAIN\",\n",
        "    \"NEURONS\", \"MOUSE\", \"MICE\",\n",
        "\n",
        "    \"OF\", \"IN\", \"TO\", \"ON\", \"BY\", \"AS\", \"AN\", \"OR\", \"IS\", \"BE\", \"WE\",\n",
        "    \"NOT\", \"THESE\", \"HAVE\", \"HAS\", \"WITHIN\", \"FOUND\", \"US\", \"INCREASE\", \"IMPACT\"\n",
        "])\n",
        "\n",
        "\n",
        "# regex patterns as a fallback\n",
        "REGEX_PATTERNS = [\n",
        "    r\"\\bC\\d+ORF\\d+\\b\",           # ex: C9ORF72\n",
        "    r\"\\bRS\\d{3,9}\\b\",              # SNP ids: rs123456\n",
        "    r\"\\b[A-Z]{2,4}-\\d{1,3}\\b\",    # ex: ABC-1, TDP-43\n",
        "    r\"\\b[A-Z]{3,6}[0-9]{0,3}\\b\"  # ex: SOD1, TP53\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPHg_jdxNNnE"
      },
      "source": [
        "# Gene extraction using NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV1FWuXZNWdm"
      },
      "outputs": [],
      "source": [
        "def extract_genes_unbiased(text):\n",
        "    \"\"\"\n",
        "    - Extract gene candidates from a text (NER when available and regex as fallback).\n",
        "\n",
        "    - Receive the original text (with capital letters/punctuation) to maximize NER retrieval.\n",
        "\n",
        "    - Return an ordered list of gene-like symbols/tokens in capital letters.\n",
        "    \"\"\"\n",
        "\n",
        "    if not text:\n",
        "        return []\n",
        "    text_str = str(text)\n",
        "    text_upper = text_str.upper()\n",
        "    genes = set()\n",
        "\n",
        "    # NER in the original text\n",
        "    if nlp_ner is not None:\n",
        "        try:\n",
        "            doc = nlp_ner(text_str)\n",
        "            for ent in doc.ents:\n",
        "                label = getattr(ent, 'label_', '')\n",
        "\n",
        "                # NER labels (depends on the model)\n",
        "                if 'GENE' in label.upper() or 'PROTEIN' in label.upper() or 'GENE_PRODUCT' in label.upper():\n",
        "                    norm = re.sub(r'[^A-Za-z0-9]', '', ent.text)\n",
        "                    norm_up = norm.upper()\n",
        "                    if 3 <= len(norm_up) <= 10 and norm_up not in GENE_STOPWORDS: # filter stopwords\n",
        "                        genes.add(norm_up)\n",
        "        except Exception:\n",
        "\n",
        "            # if NER fails, ignore it and continue with regex\n",
        "            pass\n",
        "\n",
        "\n",
        "    # regex patterns\n",
        "    for pattern in REGEX_PATTERNS:\n",
        "        for match in re.findall(pattern, text_upper):\n",
        "\n",
        "            # filter stopwords and tokens too short\n",
        "            if match and match not in GENE_STOPWORDS and len(re.sub(r'[^A-Z0-9]', '', match)) >= 3:\n",
        "                genes.add(match)\n",
        "\n",
        "\n",
        "    # removes tokens that are only numbers\n",
        "    cleaned = set()\n",
        "    for g in genes:\n",
        "        if re.search(r'[A-Z]', g):\n",
        "            cleaned.add(g)\n",
        "    return sorted(cleaned)\n",
        "\n",
        "\n",
        "# validation with mygene\n",
        "def validate_genes_with_mygene(candidate_genes):\n",
        "    \"\"\"\n",
        "    Validates a list of symbols using mygene (batch). Returns a set of validated symbols.\n",
        "\n",
        "    \"\"\"\n",
        "    if not MYGENE_AVAILABLE:\n",
        "        print(\"mygene not available; skipping validation.\")\n",
        "        return set()\n",
        "\n",
        "    mg = mygene.MyGeneInfo()\n",
        "    validated = set()\n",
        "    candidates = list(candidate_genes)\n",
        "\n",
        "    for i in range(0, len(candidates), MYGENE_BATCH_SIZE):\n",
        "        batch = candidates[i:i+MYGENE_BATCH_SIZE]\n",
        "        try:\n",
        "            res = mg.querymany(batch, scopes=['symbol', 'alias', 'name'], fields='symbol,taxid', species='human', entrezonly=False)\n",
        "            for r in res:\n",
        "                # r can signal notfound\n",
        "                if r is None:\n",
        "                    continue\n",
        "\n",
        "                if isinstance(r, dict) and not r.get('notfound', False):\n",
        "                    sym = r.get('symbol')\n",
        "                    taxid = r.get('taxid')\n",
        "\n",
        "                    # human (taxid 9606) or None (some results doesn't have taxid)\n",
        "                    if sym and (taxid is None or int(taxid) == 9606):\n",
        "                        validated.add(sym.upper())\n",
        "        except Exception as e:\n",
        "            print(f\"mygene query batch failed: {e}\")\n",
        "\n",
        "            # in case of error, we just continue\n",
        "            continue\n",
        "\n",
        "    return validated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV9PqoMCOgmB"
      },
      "source": [
        "# Pubmed article collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt3syVzjSQ1m"
      },
      "outputs": [],
      "source": [
        "\n",
        "def safe_read_abstract(article):\n",
        "    try:\n",
        "        art = article['MedlineCitation']['Article']\n",
        "        abstract_field = art.get('Abstract')\n",
        "        if not abstract_field:\n",
        "            return ''\n",
        "        abstract_text = abstract_field.get('AbstractText')\n",
        "        if not abstract_text:\n",
        "            return ''\n",
        "        abstract_parts = []\n",
        "        for a in abstract_text:\n",
        "            if isinstance(a, dict):\n",
        "                txt = a.get('#text') or a.get('label') or a.get('Label') or ''\n",
        "                abstract_parts.append(str(txt))\n",
        "            else:\n",
        "                abstract_parts.append(str(a))\n",
        "        return ' '.join([p for p in abstract_parts if p])\n",
        "    except Exception:\n",
        "        return ''\n",
        "\n",
        "def get_als_genetic_articles(query, start_year, end_year, max_articles=20000):\n",
        "    all_articles = []\n",
        "    try:\n",
        "        handle = Entrez.esearch(\n",
        "            db=\"pubmed\",\n",
        "            term=query,\n",
        "            retmax=0,\n",
        "            mindate=str(start_year),\n",
        "            maxdate=str(end_year),\n",
        "            datetype=\"pdat\",\n",
        "            api_key=API_KEY\n",
        "        )\n",
        "        result = Entrez.read(handle)\n",
        "        handle.close()\n",
        "        total = int(result.get(\"Count\", 0))\n",
        "        print(f\"Found {total} articles between {start_year}-{end_year}\")\n",
        "\n",
        "        if total == 0:\n",
        "            return []\n",
        "\n",
        "        for retstart in range(0, min(total, max_articles), 10000):\n",
        "            for retry in range(MAX_RETRIES):\n",
        "                try:\n",
        "                    handle = Entrez.esearch(\n",
        "                        db=\"pubmed\",\n",
        "                        term=query,\n",
        "                        retmax=10000,\n",
        "                        retstart=retstart,\n",
        "                        mindate=str(start_year),\n",
        "                        maxdate=str(end_year),\n",
        "                        datetype=\"pdat\",\n",
        "                        api_key=API_KEY\n",
        "                    )\n",
        "                    search_result = Entrez.read(handle)\n",
        "                    handle.close()\n",
        "                    id_list = search_result.get(\"IdList\", [])\n",
        "\n",
        "                    for i in tqdm(range(0, len(id_list), CHUNK_SIZE)):\n",
        "                        batch = id_list[i:i+CHUNK_SIZE]\n",
        "                        fetch_handle = Entrez.efetch(\n",
        "                            db=\"pubmed\",\n",
        "                            id=batch,\n",
        "                            retmode=\"xml\",\n",
        "                            api_key=API_KEY\n",
        "                        )\n",
        "                        try:\n",
        "                            data = Entrez.read(fetch_handle)\n",
        "                        except Exception:\n",
        "                            data = {}\n",
        "                        finally:\n",
        "                            fetch_handle.close()\n",
        "\n",
        "                        for article in data.get('PubmedArticle', []):\n",
        "                            try:\n",
        "                                title = article['MedlineCitation']['Article'].get('ArticleTitle', '')\n",
        "                                abstract = safe_read_abstract(article)\n",
        "                                pmid = str(article['MedlineCitation']['PMID'])\n",
        "                                text = f\"{title} {abstract}\".strip()\n",
        "                                all_articles.append({\n",
        "                                    \"pmid\": pmid,\n",
        "                                    \"title\": title,\n",
        "                                    \"abstract\": abstract,\n",
        "                                    \"text\": text\n",
        "                                })\n",
        "                            except KeyError:\n",
        "                                continue\n",
        "                        time.sleep(SLEEP_TIME)\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Attempt {retry+1} failed: {e}\")\n",
        "                    time.sleep(2 ** retry)\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error in PubMed query: {e}\")\n",
        "    return all_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxADKaG3Sadm"
      },
      "source": [
        "# Useful functions\n",
        "\n",
        "- For training models and calculating the ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBxYroQxlKT-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fasttext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from collections import defaultdict\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "def get_embeddings(gene_list, model, model_type):\n",
        "\n",
        "    \"\"\"\n",
        "      gets embeddings for gene list depending on the model\n",
        "    \"\"\"\n",
        "\n",
        "    # convert genes to lowercase for embedding lookup\n",
        "    lowercase_genes = [g.lower() for g in gene_list]\n",
        "\n",
        "    if model_type == 'fasttext':\n",
        "        # filter genes that exist in fasttext vocabulary\n",
        "        valid_genes = [g for g in lowercase_genes if model.get_word_id(g) != -1]\n",
        "        embeddings = np.array([model.get_word_vector(g) for g in valid_genes])\n",
        "\n",
        "        # return original case genes with their embeddings\n",
        "        original_case_genes = [gene_list[i] for i, g in enumerate(lowercase_genes) if g in valid_genes]\n",
        "        return embeddings, original_case_genes\n",
        "\n",
        "    elif model_type == 'word2vec':\n",
        "        valid_genes = [g for g in lowercase_genes if g in model.wv]\n",
        "        embeddings = np.array([model.wv[g] for g in valid_genes])\n",
        "\n",
        "        original_case_genes = [gene_list[i] for i, g in enumerate(lowercase_genes) if g in valid_genes]\n",
        "        return embeddings, original_case_genes\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "\n",
        "def calculate_ranking_cosine(genes, model, model_type: str, known_als_genes=None):\n",
        "\n",
        "\n",
        "\n",
        "    # default known ALS genes if none provided\n",
        "    if known_als_genes is None:\n",
        "        known_als_genes = {\n",
        "            \"ANXA11\", \"C9ORF72\", \"CHCHD10\", \"EPHA4\", \"FUS\", \"HNRNPA1\", \"KIF5A\", \"NEK1\",\n",
        "            \"OPTN\", \"PFN1\", \"SOD1\", \"TARDBP\", \"TDP-43\", \"TDP43\", \"TBK1\", \"UBQLN2\",\n",
        "            \"UNC13A\", \"VAPB\", \"VCP\"\n",
        "        }\n",
        "\n",
        "    known_als_genes = {g.upper() for g in known_als_genes}\n",
        "\n",
        "    # get embeddings for known ALS genes\n",
        "    known_embeddings, valid_known_genes = get_embeddings(\n",
        "        list(known_als_genes), model, model_type\n",
        "    )\n",
        "\n",
        "    if len(valid_known_genes) == 0:\n",
        "        print(\"No known genes found in model.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    known_embeddings = normalize(known_embeddings, axis=1) # normalize it\n",
        "\n",
        "    # get embeddings for candidate genes\n",
        "    #candidates = [gene.upper() for gene in gene_score_dict]\n",
        "    candidates = [gene.upper() for gene in genes]\n",
        "\n",
        "    candidate_embeddings, valid_candidates = get_embeddings(\n",
        "        candidates, model, model_type\n",
        "    )\n",
        "\n",
        "    if len(valid_candidates) == 0:\n",
        "        print(\"No candidate embeddings generated.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    candidate_embeddings = normalize(candidate_embeddings, axis=1) # normalize it\n",
        "\n",
        "    # cosine similarity between candidates and known genes\n",
        "    similarity_matrix = cosine_similarity(candidate_embeddings, known_embeddings)\n",
        "\n",
        "\n",
        "    # # here we are preventing a known gene to be compared to itself (if it's also a candidate)\n",
        "    # known_gene_to_idx = {gene.upper(): i for i, gene in enumerate(valid_known_genes)}\n",
        "\n",
        "\n",
        "    # for i, candidate_gene in enumerate(valid_candidates):\n",
        "\n",
        "    #     if candidate_gene.upper() in known_gene_to_idx:\n",
        "\n",
        "    #         j = known_gene_to_idx[candidate_gene.upper()]\n",
        "\n",
        "    #         similarity_matrix[i, j] = 0.0 # if the candidate is a known gene, we prevent it from comparing to itself\n",
        "\n",
        "\n",
        "    max_similarities = np.max(similarity_matrix, axis=1) # take the MAX score\n",
        "\n",
        "    # create results dataframe with combined scores\n",
        "    results_df = pd.DataFrame({\n",
        "        'gene': [g.upper() for g in valid_candidates],\n",
        "        'sim_raw': max_similarities\n",
        "    })\n",
        "\n",
        "\n",
        "    # normalize scores before combining them with alpha (just testing)\n",
        "    for col in ['sim_raw']:\n",
        "        min_val, max_val = results_df[col].min(), results_df[col].max()\n",
        "\n",
        "        results_df[f'{col}_norm'] = (results_df[col] - min_val) / (max_val - min_val + 1e-9)\n",
        "\n",
        "    return results_df.sort_values('sim_raw', ascending=False)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_ranking_dot_product(genes, model, model_type: str):\n",
        "\n",
        "\n",
        "    # normalize gene names\n",
        "    candidates = [g.lower() for g in genes]\n",
        "\n",
        "    # get candidate embeddings\n",
        "    candidate_embeddings, valid_candidates = get_embeddings(candidates, model, model_type)\n",
        "    if len(valid_candidates) == 0:\n",
        "        print(\"No candidate gene found in model.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    candidate_embeddings = normalize(candidate_embeddings, axis=1)\n",
        "\n",
        "    # \"ALS\" embedding\n",
        "    als_word = \"als\"\n",
        "    if model_type == 'fasttext':\n",
        "        if model.get_word_id(als_word) == -1:\n",
        "            print(\"'ALS' not found in fastText vocab\")\n",
        "            return pd.DataFrame()\n",
        "        als_embedding = model.get_word_vector(als_word)\n",
        "\n",
        "    elif model_type == 'word2vec':\n",
        "        if als_word not in model.wv:\n",
        "            print(\"'ALS' not found in word2vec vocab\")\n",
        "            return pd.DataFrame()\n",
        "        als_embedding = model.wv[als_word]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "    als_embedding = als_embedding / np.linalg.norm(als_embedding)  # normalize\n",
        "\n",
        "    # calculate dot product with ALS\n",
        "    dot_scores = candidate_embeddings @ als_embedding\n",
        "\n",
        "    # returns the results in a df\n",
        "    results_df = pd.DataFrame({\n",
        "        'gene': [g.upper() for g in valid_candidates],\n",
        "        'dot_with_als': dot_scores\n",
        "    })\n",
        "\n",
        "    results_df = results_df.sort_values('dot_with_als', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fB9q0kTkK4K"
      },
      "source": [
        "# Helpful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qelY7JMQcD_b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# txt for fasttext\n",
        "def save_corpus_for_fasttext(df, filepath=\"fasttext_corpus.txt\"):\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        for text in df['clean_text']:\n",
        "            f.write(text + \"\\n\")\n",
        "\n",
        "    print(f\"Corpus saved to {filepath}\")\n",
        "\n",
        "\n",
        "\n",
        "def get_word2vec_model(df, corpus_path=\"word2vec_corpus.txt\", model_path=f\"word2vec_model{YEAR_END}.bin\"):\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading existing Word2Vec model from {model_path}...\")\n",
        "        model = Word2Vec.load(model_path)\n",
        "\n",
        "    else:\n",
        "        print(\"Training new Word2Vec model...\")\n",
        "\n",
        "        # Build sentences for training\n",
        "        sentences = [simple_preprocess(text) for text in df['clean_text']]\n",
        "\n",
        "        model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            vector_size=300,  # embedding dimension\n",
        "            window=5,         # context window\n",
        "            min_count=2,      # ignore words that appear < 2 times\n",
        "            workers=4,\n",
        "            sg=1              # skip-gram\n",
        "        )\n",
        "\n",
        "        model.save(model_path)\n",
        "        print(f\"Word2Vec model trained and saved to {model_path}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train or load fastText model\n",
        "def get_fasttext_model(corpus_path=\"fasttext_corpus.txt\", model_path=f\"fasttext_model_{YEAR_END}.bin\", force_retrain=False):\n",
        "\n",
        "    if not force_retrain and os.path.exists(model_path):\n",
        "        print(f\"Loading pre-trained model from {model_path}\")\n",
        "        return fasttext.load_model(model_path)\n",
        "\n",
        "    print(\"Training new FastText model...\")\n",
        "    model = fasttext.train_unsupervised(\n",
        "        corpus_path,\n",
        "        model='skipgram',\n",
        "        dim=300,\n",
        "        epoch=10,\n",
        "        minn=3,\n",
        "        maxn=6,\n",
        "        ws=10,\n",
        "        lr=0.05,\n",
        "        minCount=2, # adjust to 2\n",
        "        thread=4)\n",
        "\n",
        "    model.save_model(model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdi4vR5JBlnM"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf7a0tLxBoVs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# precision at k - fraction of relevant items in top k results\n",
        "def calculate_precision_at_k(ranked_list, validation_set, k):\n",
        "    top_k = ranked_list[:k]\n",
        "    hits = len(set(top_k) & validation_set)\n",
        "\n",
        "    return hits / k if k > 0 else 0\n",
        "\n",
        "# recall at k - fraction of relevant items found in top k\n",
        "def calculate_recall_at_k(ranked_list, validation_set, k):\n",
        "\n",
        "    top_k = ranked_list[:k]\n",
        "    hits = len(set(top_k) & validation_set)\n",
        "\n",
        "    return hits / len(validation_set) if len(validation_set) > 0 else 0\n",
        "\n",
        "# mean reciprocal rank of first relevant item\n",
        "def calculate_mrr(ranked_list, validation_set):\n",
        "\n",
        "    for i, item in enumerate(ranked_list):\n",
        "        if item in validation_set:\n",
        "            return 1 / (i + 1)\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuEEePqmFUJD"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKHXPyvLQlOH",
        "outputId": "65bee0f0-f34d-4e75-a9cb-88f24b0244d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline...\n",
            "\n",
            "Existing data loaded from ./drive/MyDrive/IC_2025/als_articles_2010_3173_biased_updated.csv (3173 articles).\n",
            "Processed data saved to './drive/MyDrive/IC_2025/als_articles_2010_3173_biased_updated.csv'.\n",
            "Initializing fastText...\n",
            "Loading pre-trained model from ./drive/MyDrive/IC_2025/fasttext_model_2010_3173_biased_updated.bin\n",
            "\n",
            "--- TOP 20 NOVEL CANDIDATES (Model: FASTTEXT) ---\n",
            "1. VAPA       | Sim: 0.7519)\n",
            "2. NIPA1      | Sim: 0.7262)\n",
            "3. BSCL2      | Sim: 0.6360)\n",
            "4. ATF6       | Sim: 0.5952)\n",
            "5. HSPB8      | Sim: 0.5705)\n",
            "6. DBP        | Sim: 0.5595)\n",
            "7. CPA1       | Sim: 0.5589)\n",
            "8. HSPB1      | Sim: 0.5445)\n",
            "9. EEA1       | Sim: 0.5414)\n",
            "10. RCC1       | Sim: 0.5411)\n",
            "11. KIFAP3     | Sim: 0.5406)\n",
            "12. UCHL1      | Sim: 0.5295)\n",
            "13. RHOA       | Sim: 0.5252)\n",
            "14. RTN1       | Sim: 0.5251)\n",
            "15. FUSE       | Sim: 0.5217)\n",
            "16. PLEKHG5    | Sim: 0.5213)\n",
            "17. DDX20      | Sim: 0.5119)\n",
            "18. SQSTM1     | Sim: 0.4945)\n",
            "19. CHMP2B     | Sim: 0.4877)\n",
            "20. CHRNA3     | Sim: 0.4820)\n",
            "\n",
            "Results with novel genes saved to 'als_novel_gene_candidates_fasttext.csv'\n",
            "\n",
            "--- TOP 20 GENES RANKED BY DOT PRODUCT WITH 'ALS' (Model: FASTTEXT) ---\n",
            "1. KCNJ10     | Dot with ALS: 0.3943\n",
            "2. PVR        | Dot with ALS: 0.3911\n",
            "3. NOTCH3     | Dot with ALS: 0.3817\n",
            "4. MASP2      | Dot with ALS: 0.3711\n",
            "5. TBC1D1     | Dot with ALS: 0.3694\n",
            "6. IGFALS     | Dot with ALS: 0.3682\n",
            "7. ITPR2      | Dot with ALS: 0.3666\n",
            "8. CST3       | Dot with ALS: 0.3629\n",
            "9. UBE2H      | Dot with ALS: 0.3552\n",
            "10. CGA        | Dot with ALS: 0.3501\n",
            "11. FMO1       | Dot with ALS: 0.3499\n",
            "12. SOD1       | Dot with ALS: 0.3465\n",
            "13. EPO        | Dot with ALS: 0.3452\n",
            "14. FGGY       | Dot with ALS: 0.3428\n",
            "15. OPTN       | Dot with ALS: 0.3379\n",
            "16. ALS7       | Dot with ALS: 0.3348\n",
            "17. ALS3       | Dot with ALS: 0.3326\n",
            "18. KIFAP3     | Dot with ALS: 0.3287\n",
            "19. MT3        | Dot with ALS: 0.3236\n",
            "20. XBP1       | Dot with ALS: 0.3184\n",
            "\n",
            "--- TOP known genes GENES RANKED BY DOT PRODUCT WITH 'ALS' (Model: FASTTEXT) ---\n",
            "12. SOD1       | Dot with ALS: 0.3465\n",
            "15. OPTN       | Dot with ALS: 0.3379\n",
            "22. KIF5A      | Dot with ALS: 0.3180\n",
            "61. TARDBP     | Dot with ALS: 0.2787\n",
            "136. FUS        | Dot with ALS: 0.2446\n",
            "194. VAPB       | Dot with ALS: 0.2172\n",
            "300. VCP        | Dot with ALS: 0.1766\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score, ndcg_score\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import ast\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "MODEL_CHOICE = 'fasttext' # choose the model: 'fasttext' or 'word2vec'\n",
        "\n",
        "MODEL_MAP = {} # we may include more models in this dict\n",
        "\n",
        "VALIDATION_GENES = {\n",
        "    \"ANXA11\", \"C9ORF72\", \"CHCHD10\", \"EPHA4\", \"FUS\", \"HNRNPA1\", \"KIF5A\", \"NEK1\",\n",
        "    \"OPTN\", \"PFN1\", \"SOD1\", \"TARDBP\", \"TDP-43\", \"TDP43\", \"TBK1\", \"UBQLN2\",\n",
        "    \"UNC13A\", \"VAPB\", \"VCP\"\n",
        "}\n",
        "VALIDATION_GENES = {g.upper() for g in VALIDATION_GENES}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"Starting pipeline...\\n\")\n",
        "\n",
        "    # data loading and processing\n",
        "    csv_path = f\"./drive/MyDrive/IC_2025/als_articles_2010_3173_biased_updated.csv\"\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    if os.path.exists(csv_path):\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            if not df.empty and isinstance(df['genes'].iloc[0], str):\n",
        "                 df['genes'] = df['genes'].apply(lambda x: eval(x))\n",
        "\n",
        "                # convert string to list\n",
        "                #  df['genes'] = df['genes'].apply(ast.literal_eval)\n",
        "\n",
        "                # get only articles that contain genes --> filtering articles with no genes\n",
        "                 df = df[df['genes'].apply(len) > 0]\n",
        "\n",
        "\n",
        "\n",
        "            print(f\"Existing data loaded from {csv_path} ({len(df)} articles).\")\n",
        "        except (pd.errors.EmptyDataError, pd.errors.ParserError):\n",
        "            # df = get_genetic_als_articles()\n",
        "            print(\"Error: the dataframe is empty or this is another unknown error\")\n",
        "\n",
        "    else:\n",
        "\n",
        "        articles_2000_2001 = get_als_genetic_articles(BASE_QUERY, 2000, 2001, max_articles=10000)\n",
        "        articles_2001_2002 = get_als_genetic_articles(BASE_QUERY, 2001, 2002, max_articles=10000)\n",
        "        articles_2002_2003 = get_als_genetic_articles(BASE_QUERY, 2002, 2003, max_articles=10000)\n",
        "        articles_2003_2004 = get_als_genetic_articles(BASE_QUERY, 2003, 2004, max_articles=10000)\n",
        "        articles_2004_2005 = get_als_genetic_articles(BASE_QUERY, 2004, 2005, max_articles=10000)\n",
        "        articles_2005_2006 = get_als_genetic_articles(BASE_QUERY, 2005, 2006, max_articles=10000)\n",
        "        articles_2006_2007 = get_als_genetic_articles(BASE_QUERY, 2006, 2007, max_articles=10000)\n",
        "        articles_2007_2008 = get_als_genetic_articles(BASE_QUERY, 2007, 2008, max_articles=10000)\n",
        "        articles_2008_2009 = get_als_genetic_articles(BASE_QUERY, 2008, 2009, max_articles=10000)\n",
        "        articles_2009_2010 = get_als_genetic_articles(BASE_QUERY, 2009, 2010, max_articles=10000)\n",
        "        # articles_2010_2011 = get_als_genetic_articles(BASE_QUERY, 2010, 2011, max_articles=10000)\n",
        "        # articles_2011_2012 = get_als_genetic_articles(BASE_QUERY, 2011, 2012, max_articles=10000)\n",
        "        # articles_2012_2013 = get_als_genetic_articles(BASE_QUERY, 2012, 2013, max_articles=10000)\n",
        "        # articles_2013_2014 = get_als_genetic_articles(BASE_QUERY, 2013, 2014, max_articles=10000)\n",
        "        # articles_2014_2015 = get_als_genetic_articles(BASE_QUERY, 2014, 2015, max_articles=10000)\n",
        "        # articles_2015_2016 = get_als_genetic_articles(BASE_QUERY, 2015, 2016, max_articles=10000)\n",
        "        # articles_2016_2017 = get_als_genetic_articles(BASE_QUERY, 2016, 2017, max_articles=10000)\n",
        "        # articles_2017_2018 = get_als_genetic_articles(BASE_QUERY, 2017, 2018, max_articles=10000)\n",
        "        # articles_2018_2019 = get_als_genetic_articles(BASE_QUERY, 2018, 2019, max_articles=10000)\n",
        "        # articles_2019_2020 = get_als_genetic_articles(BASE_QUERY, 2019, 2020, max_articles=10000)\n",
        "        # articles_2020_2021 = get_als_genetic_articles(BASE_QUERY, 2020, 2021, max_articles=10000)\n",
        "        # articles_2021_2022 = get_als_genetic_articles(BASE_QUERY, 2021, 2022, max_articles=10000)\n",
        "        # articles_2022_2023 = get_als_genetic_articles(BASE_QUERY, 2022, 2023, max_articles=10000)\n",
        "        # articles_2023_2024 = get_als_genetic_articles(BASE_QUERY, 2023, 2024, max_articles=10000)\n",
        "        # articles_2024_2025 = get_als_genetic_articles(BASE_QUERY, 2024, 2025, max_articles=10000)\n",
        "\n",
        "        # df = pd.DataFrame(\n",
        "        #     articles_2000_2001 + articles_2001_2002 + articles_2002_2003 + articles_2003_2004 +\n",
        "        #     articles_2004_2005 + articles_2005_2006 + articles_2006_2007 + articles_2007_2008 +\n",
        "        #     articles_2008_2009 + articles_2009_2010 + articles_2010_2011 + articles_2011_2012 +\n",
        "        #     articles_2012_2013 + articles_2013_2014 + articles_2014_2015 + articles_2015_2016 +\n",
        "        #     articles_2016_2017 + articles_2017_2018 + articles_2018_2019 + articles_2019_2020 +\n",
        "        #     articles_2020_2021 + articles_2021_2022 + articles_2022_2023 + articles_2023_2024 +\n",
        "        #     articles_2024_2025\n",
        "        # )\n",
        "        df = pd.DataFrame(\n",
        "            articles_2000_2001 + articles_2001_2002 + articles_2002_2003 + articles_2003_2004 +\n",
        "            articles_2004_2005 + articles_2005_2006 + articles_2006_2007 + articles_2007_2008 +\n",
        "            articles_2008_2009 + articles_2009_2010\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #df.to_csv(\"pubmed_articles.csv\", index=False)\n",
        "        df.to_csv(f\"./drive/MyDrive/IC_2025/als_articles_2010_{len(df)}_biased_updated.csv\", index=False)\n",
        "        print(\"Total collected:\", len(df))\n",
        "\n",
        "\n",
        "    # extracting and validating genes\n",
        "    if not df.empty:\n",
        "        if 'clean_text' not in df.columns:\n",
        "            df['text'] = df['text'].fillna('')\n",
        "            df['clean_text'] = df['text'].apply(clean_text)\n",
        "        if 'genes' not in df.columns:\n",
        "            df['genes'] = df['text'].apply(extract_genes_unbiased)\n",
        "\n",
        "            if VALIDATION_GENES and MYGENE_AVAILABLE:\n",
        "                valid_genes = validate_genes_with_mygene(set(g for gl in df['genes'] for g in gl))\n",
        "                df['genes'] = df['genes'].apply(lambda genes: [g for g in genes if g in valid_genes])\n",
        "\n",
        "\n",
        "        df.to_csv(csv_path, index=False)\n",
        "\n",
        "        print(f\"Processed data saved to '{csv_path}'.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # loading/training the model\n",
        "        embedding_model = None\n",
        "\n",
        "        if MODEL_CHOICE == 'fasttext':\n",
        "            print(\"Initializing fastText...\")\n",
        "\n",
        "            corpus_filepath = f\"fasttext_corpus_2010.txt\"\n",
        "            model_filepath = f\"./drive/MyDrive/IC_2025/fasttext_model_2010_3173_biased_updated.bin\"\n",
        "\n",
        "            if not os.path.exists(corpus_filepath):\n",
        "                save_corpus_for_fasttext(df, filepath=corpus_filepath)\n",
        "\n",
        "            embedding_model = get_fasttext_model(corpus_path=corpus_filepath, model_path=model_filepath)\n",
        "\n",
        "        elif MODEL_CHOICE == 'word2vec':\n",
        "            print(\"Initializing word2vec...\")\n",
        "\n",
        "\n",
        "            corpus_filepath = \"fasttext_corpus.txt\" # change for a general txt\n",
        "            model_filepath = f\"./drive/MyDrive/IC_2025/word2vec_model_2010_3173_biased_updated.bin\"\n",
        "\n",
        "            if not os.path.exists(corpus_filepath):\n",
        "                save_corpus_for_fasttext(df, filepath=corpus_filepath)\n",
        "\n",
        "            embedding_model = get_word2vec_model(df, corpus_path=corpus_filepath, model_path=model_filepath)\n",
        "\n",
        "\n",
        "        # ranking\n",
        "        if embedding_model:\n",
        "\n",
        "            all_genes = list({g.upper() for gene_list in df['genes'] for g in gene_list})\n",
        "\n",
        "\n",
        "            ranked_genes_full = calculate_ranking_cosine(\n",
        "                genes=all_genes,\n",
        "                model=embedding_model,\n",
        "                model_type=MODEL_CHOICE,\n",
        "                known_als_genes=VALIDATION_GENES\n",
        "            )\n",
        "\n",
        "            ranked_dot_product = calculate_ranking_dot_product(\n",
        "                genes=all_genes,\n",
        "                model=embedding_model,\n",
        "                model_type=MODEL_CHOICE\n",
        "            )\n",
        "\n",
        "            # # metrics calculation (on list with known genes)\n",
        "            # print(\"\\nCalculating performance metrics...\")\n",
        "\n",
        "            # y_true = ranked_genes_full['gene'].str.upper().apply(lambda x: 1 if x in VALIDATION_GENES else 0).values\n",
        "            # y_score = ranked_genes_full['combined'].values # confirm this\n",
        "            # ranked_list_full = ranked_genes_full['gene'].str.upper().tolist()\n",
        "\n",
        "            # metrics = {\n",
        "            #     'P@10': calculate_precision_at_k(ranked_list_full, VALIDATION_GENES, 10),\n",
        "            #     'P@20': calculate_precision_at_k(ranked_list_full, VALIDATION_GENES, 20),\n",
        "            #     'R@50': calculate_recall_at_k(ranked_list_full, VALIDATION_GENES, 50),\n",
        "            #     'MAP': average_precision_score(y_true, y_score),\n",
        "            #     'nDCG': ndcg_score([y_true], [y_score]),\n",
        "            #     'MRR': calculate_mrr(ranked_list_full, VALIDATION_GENES)\n",
        "            # }\n",
        "            # metrics_df = pd.DataFrame([metrics], index=[MODEL_CHOICE.capitalize()])\n",
        "\n",
        "            # filtering for novel candidates\n",
        "            ranked_novel_genes = ranked_genes_full[~ranked_genes_full['gene'].isin(VALIDATION_GENES)]\n",
        "\n",
        "\n",
        "\n",
        "            # to make temporal analysis --> use ranked_genes_full\n",
        "            # to discover new candidate genes --> use ranked_novel_genes\n",
        "\n",
        "            if not ranked_novel_genes.empty:\n",
        "                print(f\"\\n--- TOP 20 NOVEL CANDIDATES (Model: {MODEL_CHOICE.upper()}) ---\")\n",
        "                for i, row in enumerate(ranked_novel_genes.head(20).itertuples(), 1):\n",
        "                    print(f\"{i}. {row.gene.upper():<10} | Sim: {row.sim_raw_norm:.4f})\")\n",
        "\n",
        "                output_filename = f'als_novel_gene_candidates_{MODEL_CHOICE}.csv'\n",
        "                ranked_novel_genes.to_csv(output_filename, index=False)\n",
        "                print(f\"\\nResults with novel genes saved to '{output_filename}'\")\n",
        "\n",
        "\n",
        "            if not ranked_dot_product.empty:\n",
        "                print(f\"\\n--- TOP 20 GENES RANKED BY DOT PRODUCT WITH 'ALS' (Model: {MODEL_CHOICE.upper()}) ---\")\n",
        "                for i, row in enumerate(ranked_dot_product.head(20).itertuples(), 1):\n",
        "                    print(f\"{i}. {row.gene:<10} | Dot with ALS: {row.dot_with_als:.4f}\")\n",
        "\n",
        "                ranked_dot_validation = ranked_dot_product[\n",
        "                  ranked_dot_product['gene'].apply(lambda g: str(g).upper() in VALIDATION_GENES)\n",
        "                ]\n",
        "\n",
        "\n",
        "                if not ranked_dot_product.empty:\n",
        "                    print(f\"\\n--- TOP known genes GENES RANKED BY DOT PRODUCT WITH 'ALS' (Model: {MODEL_CHOICE.upper()}) ---\")\n",
        "                    for i, row in enumerate(ranked_dot_product.itertuples(), 1):\n",
        "                        if row.gene.upper() in VALIDATION_GENES:\n",
        "                            print(f\"{i}. {row.gene:<10} | Dot with ALS: {row.dot_with_als:.4f}\")\n",
        "                else:\n",
        "                    print(\"\\nNo validation genes were found.\")\n",
        "\n",
        "\n",
        "            # print(\"\\n--- Performance Metrics Table ---\")\n",
        "            # print(metrics_df.round(4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}